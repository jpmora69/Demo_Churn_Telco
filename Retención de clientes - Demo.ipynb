{"nbformat_minor": 2, "cells": [{"source": "# Customer Churn Prediction\n\n-------------------\n\n- Customers are considered one of the most important assets for a business\n\n- In a competitive market, companies in which the customers have numerous choices of service providers they can easily switch a service or even the provider.\n\n- Such customers are referred to as churned customers.<sup>[1](#first)</sup>\n\n### Churned Customer\n\nCustomers or subscribers who stop using a company's service.\n\n### Significance and reasons of Customer  \n### Churn <sup>[2](#second), [3](#third)</sup>\n\nCustomer churn is more often due to **bad brand experiences** rather than bad products.\n\n![customer-churn](../doc/source/images/82.png)\n\nEach year, \n\n# $62 billion\n\nis lost by U.S. companies following a bad customer experience\n\n![cost](../doc/source/images/dollars.jpeg)\n\nFor every dollar invested in improving the customer experience, businesses see\n\n# 3-5x\n\nreturn\n\nFinding new customers costs\n\n# 5x\n\nmore than keeping them\n\nRepeat customers spend\n\n# 3x\n\nmore than new ones.\n\nAnd just **20%** of them account for **80%** of a company\u2019s future profits\n\nReducing churn by **5%**, businesses can increase profits anywhere from\n\n# 25% - 125%\n\n![rise](../doc/source/images/rise.jpg)", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Pipeline", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### 1. Loading Libraries", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "!pip install --upgrade pixiedust", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "!pip install watson-machine-learning-client", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing, svm\nfrom itertools import combinations\nfrom sklearn.preprocessing import PolynomialFeatures, LabelEncoder, StandardScaler\nimport sklearn.feature_selection\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nfrom sklearn import metrics\nimport pixiedust", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "### The Dataset\n\nFrom a telecommunications company. It includes information about:  \n- Customers who left within the last month \u2013 the column is called Churn\n\n- Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n\n- Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n\n- Demographic info about customers \u2013 gender, age range, and if they have partners and dependents\n\nLink for getting the dataset: [https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-Telco-Customer-Churn.csv](https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-Telco-Customer-Churn.csv)\n\nLink for other datasets: [https://www.ibm.com/communities/analytics/watson-analytics-blog/guide-to-sample-datasets/](https://www.ibm.com/communities/analytics/watson-analytics-blog/guide-to-sample-datasets/)", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### 2. Loading Our Dataset\n\nClick on the cell below to highlight it.\n\nThen go to the `Files` section to the right of this notebook and click `Insert to code` for the data you have uploaded. Choose `Insert pandas DataFrame`.", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "customer_data = df_data_2", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Checking that everything is correct\npd.set_option('display.max_columns', 30)\ncustomer_data.head(10)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "### 3. Get some info about our Dataset and whether we have missing values", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# After running this cell we will see that we have no missing values\ncustomer_data.info()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "# Drop customerID column\ncustomer_data = customer_data.drop('customerID', axis=1)\ncustomer_data.head(5)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Convert TotalCharges column to numeric as it is detected as object\nnew_col = pd.to_numeric(customer_data.iloc[:, 18], errors='coerce')\nnew_col", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Modify our dataframe to reflect the new datatype\ncustomer_data.iloc[:, 18] = pd.Series(new_col)\ncustomer_data", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Check if we have any NaN values\ncustomer_data.isnull().values.any()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Handle missing values\nfrom sklearn.preprocessing import Imputer\n\nimp = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n\ncustomer_data.iloc[:, 18] = imp.fit_transform(customer_data.iloc[:, 18].values.reshape(-1, 1))\ncustomer_data.iloc[:, 18] = pd.Series(customer_data.iloc[:, 18])", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Check if we have any NaN values\ncustomer_data.isnull().values.any()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "customer_data.info()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "### 4. Descriptive analytics for our data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Describe columns with numerical values\npd.set_option('precision', 3)\ncustomer_data.describe()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "# Describe columns with objects\ncustomer_data.describe(exclude=np.number)", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Find correlations\ncustomer_data.corr(method='pearson')", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "### 5. Visualize our Data to understand it better", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "#### Plot Relationships", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Using Pixiedust for visualization\ndisplay(customer_data)", "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {"title": "Tenure", "aggregation": "COUNT", "clusterby": "Churn", "handlerId": "barChart", "valueFields": "MonthlyCharges", "rendererId": "bokeh", "sortby": "Keys ASC", "timeseries": "false", "keyFields": "tenure"}}, "scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "# Plot Tenure Frequency count\nsns.set(style=\"darkgrid\")\nsns.set_palette(\"hls\", 3)\nfig, ax = plt.subplots(figsize=(20,10))\nax = sns.countplot(x=\"tenure\", hue=\"Churn\", data=customer_data)", "cell_type": "code", "metadata": {"scrolled": false, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Plot Tenure Frequency count\nsns.set(style=\"darkgrid\")\nsns.set_palette(\"hls\", 3)\nfig, ax = plt.subplots(figsize=(20,10))\nax = sns.countplot(x=\"Contract\", hue=\"Churn\", data=customer_data)", "cell_type": "code", "metadata": {"scrolled": false, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Plot Tenure Frequency count\nsns.set(style=\"darkgrid\")\nsns.set_palette(\"hls\", 3)\nfig, ax = plt.subplots(figsize=(20,10))\nax = sns.countplot(x=\"TechSupport\", hue=\"Churn\", data=customer_data)", "cell_type": "code", "metadata": {"scrolled": false, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Create Grid for pairwise relationships\ngr = sns.PairGrid(customer_data, size=5, hue=\"Churn\")\ngr = gr.map_diag(plt.hist)\ngr = gr.map_offdiag(plt.scatter)\ngr = gr.add_legend()", "cell_type": "code", "metadata": {"scrolled": false, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "#### Understand Data Distribution", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Set up plot size\nfig, ax = plt.subplots(figsize=(6,6))\n\n# Attributes destribution\na = sns.boxplot(orient=\"v\", palette=\"hls\", data=customer_data.iloc[:, 18], fliersize=14)", "cell_type": "code", "metadata": {"scrolled": false, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "# Tenure data distribution\nhistogram = sns.distplot(customer_data.iloc[:, 4], hist=True)\nplt.show()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Monthly Charges data distribution\nhistogram = sns.distplot(customer_data.iloc[:, 17], hist=True)\nplt.show()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Total Charges data distribution\nhistogram = sns.distplot(customer_data.iloc[:, 18], hist=True)\nplt.show()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "### 6. Encode string values in data into numerical values", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Use pandas get_dummies\ncustomer_data_encoded = pd.get_dummies(customer_data)\ncustomer_data_encoded.head(10)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "### 7. Create Training Set and Labels ", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Create training data for non-preprocessed approach\nX_npp = customer_data.iloc[:, :-1].apply(LabelEncoder().fit_transform)\npd.DataFrame(X_npp).head(5)", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "# Create training data for that will undergo preprocessing\nX = customer_data_encoded.iloc[:, :-2]\nX.head()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Extract labels\ny_unenc = customer_data['Churn']", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Convert strings of 'yes' and 'no' to binary values of 0 or 1\nle = preprocessing.LabelEncoder()\nle.fit(y_unenc)\n\ny_le = le.transform(y_unenc)\npd.DataFrame(y_le)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "### 8. Detect outliers in numerical values", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Calculate the Z-score using median value and median absolute deviation for more robust calculations\n# Working on Monthly Charges column\nthreshold = 3\n\nmedian = np.median(X['MonthlyCharges'])\nmedian_absolute_deviation = np.median([np.abs(x - median) for x in X['MonthlyCharges']])\nmodified_z_scores = [0.6745 * (x - median) / median_absolute_deviation\n                         for x in X['MonthlyCharges']]\nresults = np.abs(modified_z_scores) > threshold\n\nprint(np.any(results))", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "# Do the same for Total Charges column but using the interquartile method\n\nquartile_1, quartile_3 = np.percentile(X['TotalCharges'], [25, 75])\niqr = quartile_3 - quartile_1\nlower_bound = quartile_1 - (iqr * 1.5)\nupper_bound = quartile_3 + (iqr * 1.5)\n\nprint(np.where((X['TotalCharges'] > upper_bound) | (X['TotalCharges'] < lower_bound)))", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "### 9. Feature Engineering", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Find interactions between current features and append them to the dataframe\ndef add_interactions(dataset):\n    # Get feature names\n    comb = list(combinations(list(dataset.columns), 2))\n    col_names = list(dataset.columns) + ['_'.join(x) for x in comb]\n    \n    # Find interactions\n    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n    dataset = poly.fit_transform(dataset)\n    dataset = pd.DataFrame(dataset)\n    dataset.columns = col_names\n    \n    # Remove interactions with 0 values\n    no_inter_indexes = [i for i, x in enumerate(list((dataset ==0).all())) if x]\n    dataset = dataset.drop(dataset.columns[no_inter_indexes], axis=1)\n    \n    return dataset", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "X_inter = add_interactions(X)\nX_inter.head(15)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Select best features\nselect = sklearn.feature_selection.SelectKBest(k=25)\nselected_features = select.fit(X_inter, y_le)\nindexes = selected_features.get_support(indices=True)\ncol_names_selected = [X_inter.columns[i] for i in indexes]\n\nX_selected = X_inter[col_names_selected]\nX_selected.head(10)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "### 10. Split our dataset into train and test datasets", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "#### Split non-preprocessed data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "X_train_npp, X_test_npp, y_train_npp, y_test_npp = train_test_split(X_npp, y_le,\\\n                                                    test_size=0.33, random_state=42)\nprint(X_train_npp.shape, y_train_npp.shape)\nprint(X_test_npp.shape, y_test_npp.shape)", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_le,\\\n                                                    test_size=0.33, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "X_test.head()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### Trying to send data to the endpoint will return predictions with probabilities", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### 11. Scale our data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Use StandardScaler\nscaler = preprocessing.StandardScaler().fit(X_train, y_train)\nX_train_scaled = scaler.transform(X_train)\n\npd.DataFrame(X_train_scaled, columns=X_train.columns).head()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "pd.DataFrame(y_train).head()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "### 12. Start building a classifier", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "#### Support Vector Macines on non-preprocessed data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "from sklearn.svm import SVC\n\n# Run classifier\nclf_svc_npp = svm.SVC(random_state=42)\nclf_svc_npp.fit(X_train_npp, y_train_npp)", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "#### Support Vector Macines on preprocessed data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Run classifier\nclf_svc = svm.SVC(random_state=42)\nclf_svc.fit(X_train_scaled, y_train)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "#### Logestic Regression on preprocessed data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "from sklearn.linear_model import LogisticRegression\n\nclf_lr = LogisticRegression()\nmodel = clf_lr.fit(X_train_scaled, y_train)\nmodel", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "#### Multilayer Perceptron (Neural Network) on preprocessed data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "from sklearn.neural_network import MLPClassifier\n\nclf_mlp = MLPClassifier(verbose=0)\nclf_mlp.fit(X_train_scaled, y_train)\n\n# Note: MLP as a NN, can use data without the feature engineering step, as the NN will handle that automatically", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "### 13. Evaluate our model", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Use the scaler fit on trained data to scale our test data\nX_test_scaled = scaler.transform(X_test)\npd.DataFrame(X_test_scaled, columns=X_train.columns).head()", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "#### Evaluate SVC on non-preprocessed data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Predict confidence scores for data\ny_score_svc_npp = clf_svc_npp.decision_function(X_test_npp)\npd.DataFrame(y_score_svc_npp)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "# Get accuracy score\nfrom sklearn.metrics import accuracy_score\ny_pred_svc_npp = clf_svc_npp.predict(X_test_npp)\nacc_svc_npp = accuracy_score(y_test_npp, y_pred_svc_npp)\nprint(acc_svc_npp)", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Get Precision vs. Recall score\nfrom sklearn.metrics import average_precision_score\naverage_precision_svc_npp = average_precision_score(y_test_npp, y_score_svc_npp)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision_svc_npp))", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "#### Evaluate SVC on preprocessed data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Get model confidence of predictions\ny_score_svc = clf_svc.decision_function(X_test_scaled)\ny_score_svc", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "# Get accuracy score\ny_pred_svc = clf_svc.predict(X_test_scaled)\nacc_svc = accuracy_score(y_test, y_pred_svc)\nprint(acc_svc)", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Get Precision vs. Recall score\naverage_precision_svc = average_precision_score(y_test, y_score_svc)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision_svc))", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "#### Evaluate Logistic Regression on preprocessed data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "y_score_lr = clf_lr.decision_function(X_test_scaled)\ny_score_lr", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "y_pred_lr = clf_lr.predict(X_test_scaled)\nacc_lr = accuracy_score(y_test, y_pred_lr)\nprint(acc_lr)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "average_precision_lr = average_precision_score(y_test, y_score_lr)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision_lr))", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "#### Evaluate MLP on preprocessed data", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "y_score_mlp = clf_mlp.predict_proba(X_test_scaled)[:, 1]\ny_score_mlp", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "y_pred_mlp = clf_mlp.predict(X_test_scaled)\nacc_mlp = accuracy_score(y_test, y_pred_mlp)\nprint(acc_mlp)", "cell_type": "code", "metadata": {"scrolled": true, "slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "average_precision_mlp = average_precision_score(y_test, y_score_mlp)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision_mlp))", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "### 14. ROC Curve and models comparisons", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# Plot SVC ROC Curve\nplt.figure(0, figsize=(20,15)).clf()\n\nfpr_svc_npp, tpr_svc_npp, thresh_svc_npp = metrics.roc_curve(y_test_npp, y_score_svc_npp)\nauc_svc_npp = metrics.roc_auc_score(y_test_npp, y_score_svc_npp)\nplt.plot(fpr_svc_npp, tpr_svc_npp, label=\"SVC Non-Processed, auc=\" + str(auc_svc_npp))\n\nfpr_svc, tpr_svc, thresh_svc = metrics.roc_curve(y_test, y_score_svc)\nauc_svc = metrics.roc_auc_score(y_test, y_score_svc)\nplt.plot(fpr_svc, tpr_svc, label=\"SVC Processed, auc=\" + str(auc_svc))\n\nfpr_mlp, tpr_mlp, thresh_mlp = metrics.roc_curve(y_test, y_score_mlp)\nauc_mlp = metrics.roc_auc_score(y_test, y_score_mlp)\nplt.plot(fpr_mlp, tpr_mlp, label=\"MLP, auc=\" + str(auc_mlp))\n\nfpr_lr, tpr_lr, thresh_lr = metrics.roc_curve(y_test, y_score_lr)\nauc_lr = metrics.roc_auc_score(y_test, y_score_lr)\nplt.plot(fpr_lr, tpr_lr, label=\"Logistic Regression, auc=\" + str(auc_lr))\n\nplt.legend(loc=0)", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "#### Bonus: Sending the trained model to the cloud and scoring through a web app", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "# This cell contains Watson Machine Learning service credentials,\n#  please replace the stars with your own credentials\n\ncredentials = {\n    \"url\": \"XXXXXXXXXX\",\n    \"access_key\": \"XXXXXXXXXX\",\n    \"username\": \"XXXXXXXXXX\",\n    \"password\": \"XXXXXXXXXX\",\n    \"instance_id\": \"XXXXXXXXXX\"\n}", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "skip"}}, "outputs": [], "execution_count": null}, {"source": "# To work with the Watson Machine Learning REST API you must generate a Bearer access token\n\nimport urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{}:{}'.format(credentials['username'], credentials['password']))\nurl = '{}/v3/identity/token'.format(credentials['url'])\nresponse = requests.get(url, headers=headers)\nml_token = 'Bearer ' + json.loads(response.text).get('token')", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "fragment"}}, "outputs": [], "execution_count": null}, {"source": "# Create an online scoring endpoint\n\nendpoint_instance = credentials['url'] + \"/v3/wml_instances/\" + credentials['instance_id']\nheader = {'Content-Type': 'application/json', 'Authorization': ml_token}\n\nresponse_get_instance = requests.get(endpoint_instance, headers=header)\nprint(response_get_instance)\nprint(response_get_instance.text)", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Create API client\n\nfrom watson_machine_learning_client import WatsonMachineLearningAPIClient\n\nclient = WatsonMachineLearningAPIClient(credentials)", "cell_type": "code", "metadata": {"slideshow": {"slide_type": "subslide"}}, "outputs": [], "execution_count": null}, {"source": "# Publish model in Watson Machine Learning repository on Cloud\n\nmodel_props = {client.repository.ModelMetaNames.AUTHOR_NAME: \"XXXXXXXXXX\", \n               client.repository.ModelMetaNames.NAME: \"XXXXXXXXXX\"}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "published_model = client.repository.store_model(model=model, meta_props=model_props, \\\n                                                training_data=X_train, training_target=y_train)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "models_details = client.repository.list_models()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Create model deployment\n\npublished_model_uid = client.repository.get_model_uid(published_model)\ncreated_deployment = client.deployments.create(published_model_uid, \"Deployment of Customer Churn Prediction Model\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Get Scoring URL\nscoring_endpoint = client.deployments.get_scoring_url(created_deployment)\n\nprint(scoring_endpoint)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Get model details and expected input\nmodel_details = client.repository.get_details(published_model_uid)\nprint(json.dumps(model_details, indent=2))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### Test the model", "cell_type": "markdown", "metadata": {}}, {"source": "# Prepare the payload to be sent to the model\npayload = {\n    \"fields\": [\n        \"tenure\",\n        \"OnlineSecurity_No\",\n        \"TechSupport_No\",\n        \"Contract_Month-to-month\",\n        \"MonthlyCharges_OnlineSecurity_No\",\n        \"MonthlyCharges_TechSupport_No\",\n        \"MonthlyCharges_Contract_Month-to-month\",\n        \"Dependents_No_OnlineSecurity_No\",\n        \"Dependents_No_TechSupport_No\",\n        \"Dependents_No_Contract_Month-to-month\",\n        \"PhoneService_Yes_Contract_Month-to-month\",\n        \"InternetService_Fiber optic_OnlineSecurity_No\",\n        \"InternetService_Fiber optic_TechSupport_No\",\n        \"InternetService_Fiber optic_Contract_Month-to-month\",\n        \"InternetService_Fiber optic_PaymentMethod_Electronic check\",\n        \"OnlineSecurity_No_OnlineBackup_No\",\n        \"OnlineSecurity_No_TechSupport_No\",\n        \"OnlineSecurity_No_Contract_Month-to-month\",\n        \"OnlineSecurity_No_PaymentMethod_Electronic check\",\n        \"OnlineBackup_No_Contract_Month-to-month\",\n        \"DeviceProtection_No_Contract_Month-to-month\",\n        \"TechSupport_No_Contract_Month-to-month\",\n        \"TechSupport_No_PaymentMethod_Electronic check\",\n        \"Contract_Month-to-month_PaperlessBilling_Yes\",\n        \"Contract_Month-to-month_PaymentMethod_Electronic check\"\n ],\n    \"values\": [\n        [20.0, 0.0, 1.0, 0.0, 60.55, 10.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\t\n ]\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Send data to the model and print results\npredictions = client.deployments.score(scoring_endpoint, payload)\nprint(json.dumps(predictions, indent=2))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### Sending data to the model\n\nSending new data (may be collected from web/mobile app) in the format the model is excpecting as shown above.  \nWe get back a response with the predicted class (1 - Customer with sent data will churn)  \nand probabilities of both classes (0 or No Curn has a probability of  1.2567231699733838e-9 which is very small, 1 or Churn has a probability of 0.9999999987432768 which means the model is confident of its prediction)\n\n![postman](../doc/source/images/sample_output.png)", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "## References:\n\n#### <a name=\"first\" id=\"first\"></a><sub>[1] https://www.sciencedirect.com/science/article/abs/pii/S0148296318301231 \"Customer churn prediction in telecommunication industry using data certainty\"</sub>  \n#### <a name=\"second\" id=\"second\"></a><sub>[2] https://www.signal.co/blog/understanding-customer-churn/ \"10 Stats Expose the Real Connection Between Customer Experience and Customer Churn\"</sub>  \n#### <a name=\"third\" id=\"third\"></a><sub>[3] https://www.pinterest.com/pin/456904324667676431/ \"Mobile Telco Churn Infographic\"</sub>  \n#### <sub>[4] https://pandas.pydata.org/pandas-docs/stable/ \"Pandas Documentation\"</sub>  \n#### <sub>[5] http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html \"Scikit-Learn Imputer\"</sub>  \n#### <sub>[6] https://github.com/ibm-watson-data-lab/pixiedust/wiki/Tutorial:-Extending-the-PixieDust-Visualization \"PixieDust Documentation\"</sub>\n#### <sub>[7] https://seaborn.pydata.org/ \"Seaborn Documentation\"</sub>\n#### <sub>[8] http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder \"Scikit-Learn LabelEncoder\"</sub>\n#### <sub>[9] http://colingorrie.github.io/outlier-detection.html \"Outlier Detection Methods\"</sub>\n#### <sub>[10] http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html#sphx-glr-auto-examples-linear-model-plot-polynomial-interpolation-py \"Scikit-Learn Polynomial\"</sub>\n#### <sub>[11] http://scikit-learn.org/stable/modules/feature_selection.html \"Scikit-Learn Feature Selection\"</sub>\n#### <sub>[12] http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler \"Scikit-Learn StandardScaler\"</sub>\n#### <sub>[13] http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC \"Scikit-Learn SVC\"</sub>\n#### <sub>[14] http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression \"Scikit-Learn Logistic Regression\"</sub>\n#### <sub>[15] http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \"Scikit-Learn MLP Classifier\"</sub>\n#### <sub>[16] http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score \"Scikit-Learn Accuracy Score\"</sub>\n#### <sub>[17] http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score \"Scikit-Learn Average Precision Score\"</sub>\n#### <sub>[18] https://www.sciencedirect.com/science/article/pii/S016786550500303X \"An introduction to ROC analysis\"</sub>\n#### <sub>[19] https://wml-api-pyclient.mybluemix.net/ \"Watson Machine Learning Client Documentation\"</sub>\n#### <sub>[20] https://dataplatform.ibm.com/docs/content/analyze-data/ml-deploy-notebook.html?context=analytics \"IBM Watson Studio Documentation-Deploy a model from a notebook\"</sub>", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5 with Spark", "name": "python3", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.4", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}